# Opening the black box of Deep Neural networks via Information

## Introduction

### Information Theory

#### Entropy

- Information entropy `H(X)` is defined as the average amount of information produced by a probabilistic stochastic source of data.

  ```
  H(X) = E[I(X)] = E[-ln(P(X))]
  ```
  
- Joint infomration entropy `H(X, Y)`

  ```
  H(X,Y) = E[I(X,Y)] = E[-ln(P(X,Y))]
  ```

- Conditional entropy `H(Y|X)` quantifies the amount of information needed to describe the outcome of a random variable `Y` given that the value of another random variable `X` is known.

  ```
  H(Y|X) = H(X,Y) - H(X)
  ```

- Mutual Information `I(X,Y)` is a measure of the mutual dependence between the two variables. More specifically, it quantifies the "amount of information" (in units such as shannons, more commonly called bits) obtained about one random variable, through the other random variable.

  ```
  I(X,Y) = H(X) - H(X|Y)
         = H(Y) - H(Y|X)
         = H(X) + H(Y) - H(X,Y)
         = H(X,Y) - H(X|Y) - H(Y|X)
  ```

- Summary

![](2017.04.29.information.bottleneck/mutual-information-relative-entropy-relation-diagram.svg)

#### Information Inequality

- Jensen's Inequality

Let $$f,g:\mathbb{R} \to \mathbb{R}$$ with $$f(\cdot)$$ convex, then $$E[f(g(X))] \ge f (E[g(X)])$$.

- The Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution diverges from a second, expected probability distribution.

KL divergence from Q to P

$$D_KL(P||Q) = \displaystyle \int_{-\infty}^{+\infty} -p(x) \log \frac{q(x)}{p(x)}$$

The Kullback–Leibler divergence can be interpreted as the expected extra message-length per datum that must be communicated if a code that is optimal for a given (wrong) distribution Q is used, compared to using a code based on the true distribution P.

$$D_KL(P||Q) = H(P,Q) - H(P)$$, where H(P,Q) is the cross entropy of P and Q, and H(P) is the entropy of P.

- $$D_KL(P||Q) \ge 0$$ 

- $$I(X,Y) = D_KL(p(x,y)||p(x)p(y)) \ge 0$$

- Data Processing Inequality (DPI):

  For any 3 variables that form a Markov chain $$X \rightarrow Y \rightarrow Z, I(X,Y) \ge I(X,Z)$$.

## Key Note


- ![](figure1) [Figure1 in paper]

  - view layered neural networks as a Markov chain of successive representations of the input layer
   
  - charateristic the network layers on the Information Plane - the plane of the Mutual Information values of any other variable with the input variable X and desired output variable Y
  
  - layer matters, rather than single neuron in a layer.
  
- ![Figure4 in paper](figure2) [Figure4 in paper]

Two different and distinct phases in Stochastic Gradient Decent
(SGD) optimization, commonly used in Deep Learning

  - 1st phase (~350 epochs): empirical error minimization (ERM)
  
    - drift phase, gradient mean >> standard deviations, high signal noise ratio (SNR)
   
  - 2nd phase: representation compression
  
    - diffusion phase, gradient mean << batch fluctuation, low signal noise ratio (SNR)
    
    - maximize the conditional entropy $$H(X|T_i)$$, or equivalently, minimize the mutual information $$I(X, T_i)$$, by additive noise, a.k.a stochastic relaxation, constrained by the empirical error.
    
  - Keep training even when error rate is low and stable -> better generalization.
  
  - [Video](https://www.youtube.com/watch?v=bLqJHjXihK8) 13:46

- ![Figure5 in paper](figure3) [Figure5 in paper]

The benefit of the hidden layers
  
  - Adding hidden layers dramatically reduces the number of training epochs for good generalization.
  
  - The optimization time depend super-linearly (exponentially?) of the compressed information $$\Delta I_x$$ for each layer.
  
    - The compression phase of each layer is shorter when it starts from a previous compressed layer.
  
    - The compression is faster for the deeper (narrower and closer to the output) layers.
  
  - Even wide hidden layers eventually compress in the diffusion phase. Adding extra width does not help.
    
- ![Figure7 in paper](figure4) [Figure7 in paper]

The effect of the training data size on the layers in the information plane.

  - As expected, with increasing training size the layers’ true label information (generalization) I Y is pushed up and gets closer to the theoretical IB bound for the rule distribution.
  
  - The layers converge to specific points on the finite sample information curves, which can be calculated using the IB self-consistent equations [Eq9 in paper].
  
  - In the lower layers, the training size hardly changes the information at all, since even random weights keep most of the mutual information on both X and Y. 
  
  - However, for the deeper layers the network learns to preserve more of the information on Y and better compress the irrelevant information in X. With larger training samples more details on X become relevant for Y and we there is a shift to higher $$I_X$$ in the middle layers. 
